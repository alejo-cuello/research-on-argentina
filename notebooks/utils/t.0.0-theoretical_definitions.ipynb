{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of missing data:\n",
    "\n",
    "**MCAR: Missing Completely At Random**  \n",
    "The missing values in the data set occur completely at random. They don't depend on any other data. Example: when a device such a security camera stops working.  \n",
    "*How to handle this kind of missing data? Apply data deletion or imputation (imputation is more recommended)*  \n",
    "\n",
    "**MAR: Missing At Random**  \n",
    "    The missing values depends on other observed values. Example: devices required a periodic maintenance to ensure consistent operation, so the data will be missing during those maintenance period.  \n",
    "    *How to handle this kind of missing data? Single or multiple imputation (consider one or several columns during imputation)*  \n",
    "\n",
    "**MNAR: Missing Not At Random**  \n",
    "    The missing values depends on the missing values themselves. They are very difficult to identify. And we may not even know that the data is missing. Example: tools have limitations. When attempting to track data out in areas beyond the measurement range, missing values are generated. Example, a scale not detecting very small or very large values.  \n",
    "    *How to handle this kind of missing data? This kind of missing values require to perform sensivity analysis. If it's not possible, imputation is preferable over deletion.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals\n",
    "Residuals of a regression model or a time serie represent the difference between observed and predicted values.\n",
    "\n",
    "*Residual = observed value - predicted value*\n",
    "\n",
    "The model will be valid if its residuals must be behave as a white noise. We apply test like Durbin Watson o Ljung Box to detect autocorrelation between residuals, i.e the errors are correlationes in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between correlation and autocorrelation\n",
    "\n",
    "## Correlation\n",
    "\n",
    "- Correlation measures the relationship between 2 different time series in the same time period\n",
    "- It indicates if two variables tend to increase or decrease together\n",
    "- It's expressed through Pearson correlation coefficient, whose value ranges between -1 (perfect negative correlation) y 1 (perfect positive correlation).\n",
    "\n",
    "For example:\n",
    "\n",
    "Daily temperature and energy consumption can be correlated\n",
    "\n",
    "```python\n",
    "df[[\"temperature\", \"energy_consumption\"]].corr()\n",
    "```\n",
    "\n",
    "## Autocorrelation\n",
    "- The autocorrelation measures the relationship of a serie to itself in different time periods.\n",
    "- It indicates if past values influence future ones.\n",
    "- It reveals patterns such a trends and seasonality\n",
    "\n",
    "For example:\n",
    "\n",
    "Electricity consumption in one day can be influenced by the previous day's consumption.\n",
    "\n",
    "```python\n",
    "    from pandas.plotting import autocorrelation_plot\n",
    "    autocorrelation_plot(df[\"energy_consumption\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTest (T-Student Test) functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scipy.stats.ttest_* functions (like ttest_ind, ttest_rel, and ttest_1samp) perform a t-test, which is a statistical hypothesis test to compare means. These functions typically return two values:\n",
    "\n",
    "* Statistic: The t-statistic is a measure of how far the sample mean is from the null hypothesis (e.g., the means are equal) in terms of standard error. A higher absolute value indicates a stronger deviation from the null hypothesis.\n",
    "* p-value: This value represents the probability of observing the data (or something more extreme) under the null hypothesis. If the p-value is below your chosen significance level (e.g., 0.05), you can reject the null hypothesis.\n",
    "\n",
    "To execute ttest_ind() properly, you need two samples that:\n",
    "\n",
    "* Are numerical (not boolean or categorical).\n",
    "* Follow an approximately normal distribution (for small sample sizes).\n",
    "* Not contain NaN values, because NaN will propagate through the calculation, leading to incorrect or nan results.\n",
    "\n",
    "## Example Explanation\n",
    "\n",
    "Suppose you run:\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Example data\n",
    "sample1 = [2.3, 3.5, 4.2, 5.6, 6.8]\n",
    "sample2 = [1.2, 2.8, 3.4, 4.7, 5.9]\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_stat, p_val = ttest_ind(sample1, sample2)\n",
    "\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_val)\n",
    "```\n",
    "\n",
    "## \"alternative\" Function Parameter\n",
    "\n",
    "Defines the alternative hypothesis. The following options are available (default is ‘two-sided’):\n",
    "- two-sided: the means of the distributions underlying the samples are unequal.\n",
    "- less: the mean of the distribution underlying the first sample is less than the mean of the distribution underlying the second sample.\n",
    "- greater: the mean of the distribution underlying the first sample is greater than the mean of the distribution underlying the second sample.\n",
    "\n",
    "## Output Interpretation\n",
    "\n",
    "- t-statistic:\n",
    "    - A positive or negative value indicating the difference between the means.\n",
    "    - A positive value means the mean of sample1 is higher than sample2.\n",
    "    - A negative value means the mean of sample1 is lower than sample2.\n",
    "    - Larger absolute values indicate greater evidence against the null hypothesis.\n",
    "\n",
    "- p-value:\n",
    "    - A small p-value (e.g., < 0.05) suggests a significant difference between the two groups.\n",
    "    - A large p-value means the evidence is insufficient to reject the null hypothesis.\n",
    "\n",
    "## Common Use Cases\n",
    "\n",
    "- ttest_ind:\n",
    "    - Used for two independent samples.\n",
    "    - Example: Comparing test scores between two different classes.\n",
    "\n",
    "- ttest_rel:\n",
    "    - Used for two related samples (paired samples).\n",
    "    - Example: Comparing the same students' test scores before and after a course.\n",
    "\n",
    "- ttest_1samp:\n",
    "    - Used to compare a sample mean to a population mean.\n",
    "    - Example: Checking if a class's average test score differs from the national average.\n",
    "\n",
    "## Assumptions of results of Ttest\n",
    "\n",
    "The results depend on assumptions:\n",
    "- Normality of data.\n",
    "- Equal variances (for ttest_ind). Use equal_var=False if variances are unequal.\n",
    "Check the assumptions before interpreting the result to ensure validity.\n",
    "- Independence between the samples\n",
    "\n",
    "You must check the underlying assumptions. Here are the common assumptions and how to test them:\n",
    "\n",
    "### 1.Normality of the Data: \n",
    "\n",
    "The data in each group should follow a normal distribution.\n",
    "\n",
    "#### Visual Inspection:\n",
    "\n",
    "Use histograms or Q-Q plots to assess if the data roughly follows a normal curve.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.hist(sample1, bins=10, alpha=0.7, label=\"Sample 1\")\n",
    "plt.hist(sample2, bins=10, alpha=0.7, label=\"Sample 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "stats.probplot(sample1, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Sample 1 Q-Q Plot\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Shapiro-Wilk Test:\n",
    "\n",
    "A formal test for normality. If the p-value is < 0.05, the data significantly deviates from normality.\n",
    "\n",
    "```python\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "stat, p = shapiro(sample1)\n",
    "print(\"Shapiro-Wilk Test - Sample 1: stat =\", stat, \", p-value =\", p)\n",
    "\n",
    "stat, p = shapiro(sample2)\n",
    "print(\"Shapiro-Wilk Test - Sample 2: stat =\", stat, \", p-value =\", p)\n",
    "```\n",
    "\n",
    "#### Kolmogorov-Smirnov Test:\n",
    "\n",
    "Another test for normality (especially for larger samples).\n",
    "\n",
    "```python\n",
    "from scipy.stats import kstest\n",
    "\n",
    "stat, p = kstest(sample1, \"norm\", args=(sample1.mean(), sample1.std()))\n",
    "print(\"K-S Test - Sample 1: stat =\", stat, \", p-value =\", p)\n",
    "```\n",
    "\n",
    "### 2.Equal Variances (Homogeneity of Variance):\n",
    "\n",
    "The variance in the two groups is similar (only for independent t-tests).\n",
    "\n",
    "#### Levene’s Test:\n",
    "\n",
    "Tests the null hypothesis that the variances are equal. If the p-value is < 0.05, the variances are significantly different.\n",
    "\n",
    "```python\n",
    "from scipy.stats import levene\n",
    "\n",
    "stat, p = levene(sample1, sample2)\n",
    "print(\"Levene's Test: stat =\", stat, \", p-value =\", p)\n",
    "```\n",
    "\n",
    "#### F-Test (Ratio of Variances):\n",
    "\n",
    "Compare the variances directly.\n",
    "\n",
    "```python\n",
    "f_stat = np.var(sample1, ddof=1) / np.var(sample2, ddof=1)\n",
    "print(\"F-statistic (Variance Ratio):\", f_stat)\n",
    "```\n",
    "If variances are unequal, use the equal_var=False parameter in ttest_ind:\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_ind\n",
    "t_stat, p_val = ttest_ind(sample1, sample2, equal_var=False)\n",
    "```\n",
    "\n",
    "### 3.Independence between the samples:\n",
    "\n",
    "Samples must be independent between them. This is more about the design of your experiment or data collection process. Verify that your data collection process ensures no overlap or dependence between groups.\n",
    "\n",
    "#### Durbin-Watson Test (for detecting autocorrelation between time series)\n",
    "\n",
    "To detect autocorrelation between residuals of a model such as linear regression\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw_statistic = durbin_watson(residuals) #  regression model residuals\n",
    "print(f\"Durbin-Watson statistic: {dw_statistic}\")\n",
    "```\n",
    "\n",
    "Output interpretation:\n",
    "    Value close to 2: There is no autocorrelation.\n",
    "    Value close to 0: Positive autocorrelation.\n",
    "    Value close to 4: Negative autocorrelation.\n",
    "\n",
    "#### Ljung-Box Test (for detecting correlation between time series)\n",
    "\n",
    "To detect correlation between different lags (previous values) of a time series\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "ljung_box_result = acorr_ljungbox(residuals, lags=[10], return_df=True) # Model regression residuals\n",
    "print(ljung_box_result)\n",
    "```\n",
    "\n",
    "If p-value is lower than 0.05, there is evidence of autorrelation\n",
    "\n",
    "#### For time-series data, the autocorrelation can be checked by:\n",
    "\n",
    "This plot is an Autocorrelation Function (ACF) plot, which shows how a time series is correlated with its own past values (lags). I\n",
    "\n",
    "```python\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plot_acf(sample1) # acf = Auto Correlation Function\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Interpretation of the Plot:\n",
    "\n",
    "* Y-Axis (Autocorrelation Coefficient, r_k)\n",
    "    * Values range from -1 to 1.\n",
    "    * A positive value means a direct relationship (past values are similar to future ones).\n",
    "    * A negative value means an inverse relationship (past values are opposite to future ones).\n",
    "\n",
    "* X-Axis (Lags)\n",
    "    * The lags indicate how many steps back in time we are looking at.\n",
    "    * The first bar at lag = 0 is always 1, since a time series is perfectly correlated with itself at lag 0.\n",
    "\n",
    "* Bars and Confidence Interval (Shaded Area)\n",
    "    * The bars represent autocorrelation coefficients at different lags.\n",
    "    * The blue shaded area is the confidence interval. If a bar is outside this region, it is statistically significant (not due to randomness).\n",
    "\n",
    "#### Correlation Matrix (For numeric variables):\n",
    "\n",
    "```python\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n",
    "```\n",
    "\n",
    "If correlation values are high (> 0.8 or < -0.8), the variables can be related.\n",
    "\n",
    "#### Chi-squared Test (For categorical variables):\n",
    "\n",
    "```python\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table between 2 categorical variables\n",
    "contingency_table = pd.crosstab(df[\"cat_1\"], df[\"cat_2\"])\n",
    "\n",
    "# Aplicar la prueba de Chi-cuadrado\n",
    "chi2_stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"P-valor: {p}\")\n",
    "```\n",
    "If p < 0.05, the variables are not independent\n",
    "\n",
    "## What If Assumptions Are Violated?\n",
    "\n",
    "- Normality:\n",
    "    - Use a non-parametric test like the Mann-Whitney U Test (scipy.stats.mannwhitneyu) for independent samples or the Wilcoxon Signed-Rank Test for paired samples.\n",
    "\n",
    "- Equal Variances:\n",
    "    - Use equal_var=False in ttest_ind (Welch’s t-test).\n",
    "\n",
    "- Independence:\n",
    "    - Consider using statistical models (e.g., mixed-effects models) to handle dependencies explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares Regression\n",
    "\n",
    "## Assumptions of results of Ttest\n",
    "\n",
    "Ordinary Least Squares (OLS) regression requires checking key assumptions to ensure valid and reliable results. Below are the main assumptions and how to test them:\n",
    "- Ensure linearity by inspecting scatter plots.\n",
    "- After fitting the model, validate assumptions with residual plots.\n",
    "- Check for multicollinearity if multiple predictors are used.\n",
    "\n",
    "### 1. Linearity\n",
    "\n",
    "The relationship between independent variables and the dependent variable is linear.\n",
    "\n",
    "#### Plot the residuals vs. fitted values\n",
    "\n",
    "If there is no clear pattern, linearity is likely satisfied.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fitted_values = model.fittedvalues\n",
    "residuals = model.resid\n",
    "\n",
    "plt.scatter(fitted_values, residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 2. Independence of Errors\n",
    "\n",
    "The residuals (errors) are independent.\n",
    "\n",
    "#### Use the Durbin-Watson statistic\n",
    "\n",
    "A value close to 2 indicates no autocorrelation.\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(\"Durbin-Watson Statistic:\", dw_stat)\n",
    "```\n",
    "\n",
    "### 3. Normality of Errors\n",
    "\n",
    "The residuals should be normally distributed.\n",
    "\n",
    "#### Use a Q-Q plot\n",
    "\n",
    "Residuals should fall along the diagonal line.\n",
    "\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Perform a Shapiro-Wilk test:\n",
    "\n",
    "```python\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "stat, p = shapiro(residuals)\n",
    "print(\"Shapiro-Wilk Test: stat =\", stat, \", p-value =\", p)\n",
    "```\n",
    "\n",
    "### 4. Homoscedasticity (Equal Variance of Errors)\n",
    "\n",
    "The variance of residuals should be constant.\n",
    "\n",
    "#### Use the Breusch-Pagan test.\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "_, pval, _, _ = het_breuschpagan(residuals, model.model.exog)\n",
    "print(\"Breusch-Pagan Test p-value:\", pval)\n",
    "```\n",
    "\n",
    "### 5. No Multicollinearity (for Multiple Regression)\n",
    "\n",
    "Independent variables should not be highly correlated.\n",
    "\n",
    "#### Compute Variance Inflation Factor (VIF). A VIF > 5 indicates multicollinearity.\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = model.model.exog\n",
    "vif = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "print(\"VIFs:\", vif)\n",
    "```\n",
    "### **Interpreting Model Output**\n",
    "\n",
    "- R-squared: The proportion of the variance in the dependent variable (temp) that is explained by the independent variable (ozone). Example: While the R-squared value of 0.488 shows that ozone explains about 48.8% of the variance in temp, there might be other variables affecting temp not included in this model.\n",
    "- Adjusted R-squared: Similar to R-squared but adjusts for the number of predictors in the model. It penalizes the addition of irrelevant predictors.\n",
    "- F-statistic: measures the overall significance of the model.\n",
    "- Prob (F-statistic): very close to 0 indicates that the model is statistically significant, meaning that independent variables has a significant effect on predicted variable.\n",
    "Log-Likelihood: A measure of the likelihood of the data under the fitted model.\n",
    "- AIC (Akaike Information Criterion): A metric for model comparison, with lower values indicating a better model.\n",
    "- BIC (Bayesian Information Criterion): Similar to AIC but includes a stronger penalty for adding more parameters to the model.\n",
    "- Number of Observations: The total number of data points used in the analysis.\n",
    "- Df Residuals: Degrees of freedom for residuals (number of observations minus the number of parameters).\n",
    "- Df Model: The number of predictors (independent variables) in the model.\n",
    "- Covariance Type: Nonrobust Indicates that standard errors and covariance calculations are not adjusted for heteroskedasticity or autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data imputation methods\n",
    "\n",
    "- If you are working with time series, you can apply:\n",
    "    - Backward fill (or backfilling)\n",
    "    - Forward fill (or forward filling)\n",
    "    - Interpolation\n",
    "- If you are not working with time series, you can apply:\n",
    "    - Constant imputation\n",
    "    - Mean imputation\n",
    "    - Median imputation\n",
    "    - Mode imputation\n",
    "\n",
    "#### What Are Donor-Based Imputations?\n",
    "\n",
    "They fill in missing values for a given unit by copying observed values from another unit, the donor.\n",
    "\n",
    "#### What Are Model-Based Imputations?\n",
    "\n",
    "The goal is to find a predictive model for each target variable in the dataset that contains missing values.\n",
    "\n",
    "### Mean, Median, and Mode Imputation\n",
    "*Advantages:*\n",
    "\n",
    "- Quick and easy\n",
    "- The mean can be useful in the presence of outliers\n",
    "- Does not affect the statistic in question or the sample size\n",
    "\n",
    "*Disadvantages:*\n",
    "\n",
    "- Can bias results since it alters the distribution (kurtosis)\n",
    "- Loses correlations between variables; not very precise\n",
    "- Cannot be used for categorical variables (except for mode)\n",
    "\n",
    "### Forward and Backward Fill Imputation\n",
    "\n",
    "*Advantages:*\n",
    "\n",
    "- Quick and easy\n",
    "- Imputed data are not constant\n",
    "- There are tricks to avoid breaking relationships between variables\n",
    "\n",
    "*Disadvantages:*\n",
    "\n",
    "- Multivariable relationships may be distorted\n",
    "\n",
    "*Functions to apply this types of imputations:*\n",
    "\n",
    "- ffill (forward fill): Imputes forward.\n",
    "- bfill (back fill): Imputes backward.\n",
    "\n",
    "For categorical variables, it is recommended to sort (use groupby instead of sort_values) the dataframe to maintain the relationship between missing values and the values of other variables (e.g., filling in a woman's height using the height of another woman).\n",
    "\n",
    "### Interpolation Imputation\n",
    "\n",
    "Different interpolation methods can be used, such as:\n",
    "\n",
    "- Straight-line interpolation: Model-based interpolation\n",
    "- Nearest neighbor interpolation: Donor-based interpolation\n",
    "\n",
    "*Advantages:*\n",
    "\n",
    "- Easy to implement\n",
    "- Useful for time series\n",
    "- Offers a variety of interpolation options\n",
    "\n",
    "*Disadvantages:*\n",
    "\n",
    "- May break relationships between variables\n",
    "- May introduce out-of-range values\n",
    "\n",
    "### k-NN (k-Nearest Neighbors) Imputation\n",
    "\n",
    "*For each observation with missing values:*\n",
    "\n",
    "- Find k most similar observations (donors, neighbors).\n",
    "- Replace missing values with the aggregated values of these k neighbors.\n",
    "\n",
    "*How to Determine the Most Similar Neighbors?*\n",
    "\n",
    "By quantifying distances:\n",
    "\n",
    "- Euclidean distance: Useful for numerical variables.\n",
    "- Manhattan distance: Useful for factor-type variables (e.g., Monday, Tuesday; slow, fast).\n",
    "- Hamming distance: Useful for categorical variables.\n",
    "- Gower distance: Useful for datasets with mixed variables (not only numerical, categorical, or factor-type).\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Easy to implement\n",
    "- Performs well with small datasets\n",
    "- Excellent for numerical data but also works with mixed data\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Scalability can be an issue\n",
    "- Requires special transformations for categorical variables\n",
    "- Sensitive to outliers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-on-argentina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
